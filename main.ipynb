{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fredericmarcotte/Code/Kaggle_INF8245/env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Iterable, Tuple\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Band Name</th>\n",
       "      <th>Band Genre</th>\n",
       "      <th>Band Country of Origin</th>\n",
       "      <th>Band Debut</th>\n",
       "      <th>Concert ID</th>\n",
       "      <th>Concert Attendance</th>\n",
       "      <th>Inside Venue</th>\n",
       "      <th>Rain</th>\n",
       "      <th>Seated</th>\n",
       "      <th>Personnality Trait 1</th>\n",
       "      <th>Personnality Trait 2</th>\n",
       "      <th>Personnality Trait 3</th>\n",
       "      <th>Personnality Trait 4</th>\n",
       "      <th>Concert Goer Age</th>\n",
       "      <th>Concert Goer ID</th>\n",
       "      <th>Height (cm)</th>\n",
       "      <th>Concert Goer Country of Origin</th>\n",
       "      <th>Concert Enjoyment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ConcertExperience_180106</td>\n",
       "      <td>Teenage Crazy Blue Knickers</td>\n",
       "      <td>Indie/Alt Rock</td>\n",
       "      <td>United States of America (USA)</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>900.0</td>\n",
       "      <td>2980.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.330843</td>\n",
       "      <td>-0.958408</td>\n",
       "      <td>-0.943548</td>\n",
       "      <td>-1.636806</td>\n",
       "      <td>29.0</td>\n",
       "      <td>concert_goer_1985</td>\n",
       "      <td>140.0</td>\n",
       "      <td>Paraguay</td>\n",
       "      <td>Did Not Enjoy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ConcertExperience_146268</td>\n",
       "      <td>Beyond Devon</td>\n",
       "      <td>Pop Music</td>\n",
       "      <td>United States of America (USA)</td>\n",
       "      <td>1968.0</td>\n",
       "      <td>731.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>-2.069449</td>\n",
       "      <td>0.017777</td>\n",
       "      <td>-1.910675</td>\n",
       "      <td>0.610265</td>\n",
       "      <td>43.0</td>\n",
       "      <td>concert_goer_1874</td>\n",
       "      <td>158.0</td>\n",
       "      <td>United Kingdom (UK)</td>\n",
       "      <td>Enjoyed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ConcertExperience_128743</td>\n",
       "      <td>Ron Talent</td>\n",
       "      <td>Rock n Roll</td>\n",
       "      <td>Canada</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>162754.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.484268</td>\n",
       "      <td>1.968772</td>\n",
       "      <td>-0.064167</td>\n",
       "      <td>-1.260871</td>\n",
       "      <td>68.0</td>\n",
       "      <td>concert_goer_442</td>\n",
       "      <td>159.0</td>\n",
       "      <td>United States of America (USA)</td>\n",
       "      <td>Did Not Enjoy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ConcertExperience_140839</td>\n",
       "      <td>Devon Revival</td>\n",
       "      <td>RnB</td>\n",
       "      <td>United States of America (USA)</td>\n",
       "      <td>1992.0</td>\n",
       "      <td>704.0</td>\n",
       "      <td>8103.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.858054</td>\n",
       "      <td>1.022827</td>\n",
       "      <td>-0.348389</td>\n",
       "      <td>-1.147251</td>\n",
       "      <td>17.0</td>\n",
       "      <td>concert_goer_1149</td>\n",
       "      <td>150.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Worst Concert Ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ConcertExperience_19149</td>\n",
       "      <td>Beyond Devon</td>\n",
       "      <td>Pop Music</td>\n",
       "      <td>United States of America (USA)</td>\n",
       "      <td>1968.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.793029</td>\n",
       "      <td>-1.166528</td>\n",
       "      <td>-0.043766</td>\n",
       "      <td>0.969661</td>\n",
       "      <td>59.0</td>\n",
       "      <td>concert_goer_930</td>\n",
       "      <td>166.0</td>\n",
       "      <td>United Kingdom (UK)</td>\n",
       "      <td>Did Not Enjoy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169995</th>\n",
       "      <td>ConcertExperience_14055</td>\n",
       "      <td>Crazy Joystick Cult</td>\n",
       "      <td>RnB</td>\n",
       "      <td>Canada</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>162754.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.095021</td>\n",
       "      <td>0.175175</td>\n",
       "      <td>0.914245</td>\n",
       "      <td>0.357359</td>\n",
       "      <td>50.0</td>\n",
       "      <td>concert_goer_707</td>\n",
       "      <td>180.0</td>\n",
       "      <td>United States of America (USA)</td>\n",
       "      <td>Did Not Enjoy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169996</th>\n",
       "      <td>ConcertExperience_192792</td>\n",
       "      <td>Crazy Joystick Cult</td>\n",
       "      <td>RnB</td>\n",
       "      <td>Canada</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>963.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-0.733719</td>\n",
       "      <td>-0.285776</td>\n",
       "      <td>-0.323312</td>\n",
       "      <td>0.641180</td>\n",
       "      <td>71.0</td>\n",
       "      <td>concert_goer_1373</td>\n",
       "      <td>143.0</td>\n",
       "      <td>Bulgaria</td>\n",
       "      <td>Worst Concert Ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169997</th>\n",
       "      <td>ConcertExperience_152942</td>\n",
       "      <td>Why Frogs, Why?</td>\n",
       "      <td>Heavy Metal</td>\n",
       "      <td>Canada</td>\n",
       "      <td>2005.0</td>\n",
       "      <td>764.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.744969</td>\n",
       "      <td>-0.965547</td>\n",
       "      <td>1.020598</td>\n",
       "      <td>1.027389</td>\n",
       "      <td>27.0</td>\n",
       "      <td>concert_goer_1286</td>\n",
       "      <td>176.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Did Not Enjoy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169998</th>\n",
       "      <td>ConcertExperience_138957</td>\n",
       "      <td>Twilight of the Joystick Gods</td>\n",
       "      <td>Hip Hop/Rap</td>\n",
       "      <td>United States of America (USA)</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>694.0</td>\n",
       "      <td>22026.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0.821976</td>\n",
       "      <td>0.351411</td>\n",
       "      <td>0.175762</td>\n",
       "      <td>1.455654</td>\n",
       "      <td>39.0</td>\n",
       "      <td>concert_goer_1845</td>\n",
       "      <td>176.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Did Not Enjoy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169999</th>\n",
       "      <td>ConcertExperience_132336</td>\n",
       "      <td>Joystick Attack</td>\n",
       "      <td>Heavy Metal</td>\n",
       "      <td>United Kingdom (UK)</td>\n",
       "      <td>2008.0</td>\n",
       "      <td>661.0</td>\n",
       "      <td>2980.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>-1.017410</td>\n",
       "      <td>0.883248</td>\n",
       "      <td>-0.184107</td>\n",
       "      <td>0.631731</td>\n",
       "      <td>46.0</td>\n",
       "      <td>concert_goer_1281</td>\n",
       "      <td>146.0</td>\n",
       "      <td>Canada</td>\n",
       "      <td>Best Concert Ever</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170000 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Id                      Band Name  \\\n",
       "0       ConcertExperience_180106    Teenage Crazy Blue Knickers   \n",
       "1       ConcertExperience_146268                   Beyond Devon   \n",
       "2       ConcertExperience_128743                     Ron Talent   \n",
       "3       ConcertExperience_140839                  Devon Revival   \n",
       "4        ConcertExperience_19149                   Beyond Devon   \n",
       "...                          ...                            ...   \n",
       "169995   ConcertExperience_14055            Crazy Joystick Cult   \n",
       "169996  ConcertExperience_192792            Crazy Joystick Cult   \n",
       "169997  ConcertExperience_152942                Why Frogs, Why?   \n",
       "169998  ConcertExperience_138957  Twilight of the Joystick Gods   \n",
       "169999  ConcertExperience_132336                Joystick Attack   \n",
       "\n",
       "            Band Genre          Band Country of Origin  Band Debut  \\\n",
       "0       Indie/Alt Rock  United States of America (USA)      1976.0   \n",
       "1            Pop Music  United States of America (USA)      1968.0   \n",
       "2          Rock n Roll                          Canada      1955.0   \n",
       "3                  RnB  United States of America (USA)      1992.0   \n",
       "4            Pop Music  United States of America (USA)      1968.0   \n",
       "...                ...                             ...         ...   \n",
       "169995             RnB                          Canada      1985.0   \n",
       "169996             RnB                          Canada      1985.0   \n",
       "169997     Heavy Metal                          Canada      2005.0   \n",
       "169998     Hip Hop/Rap  United States of America (USA)      1995.0   \n",
       "169999     Heavy Metal             United Kingdom (UK)      2008.0   \n",
       "\n",
       "        Concert ID  Concert Attendance Inside Venue   Rain Seated  \\\n",
       "0            900.0              2980.0        False  False    NaN   \n",
       "1            731.0                54.0         True  False   True   \n",
       "2              NaN            162754.0        False  False   True   \n",
       "3            704.0              8103.0        False   True  False   \n",
       "4             95.0                54.0        False  False  False   \n",
       "...            ...                 ...          ...    ...    ...   \n",
       "169995        70.0            162754.0         True  False  False   \n",
       "169996       963.0                54.0        False  False  False   \n",
       "169997       764.0                54.0        False  False  False   \n",
       "169998       694.0             22026.0        False   True   True   \n",
       "169999       661.0              2980.0         True  False  False   \n",
       "\n",
       "        Personnality Trait 1  Personnality Trait 2  Personnality Trait 3  \\\n",
       "0                   0.330843             -0.958408             -0.943548   \n",
       "1                  -2.069449              0.017777             -1.910675   \n",
       "2                  -0.484268              1.968772             -0.064167   \n",
       "3                  -0.858054              1.022827             -0.348389   \n",
       "4                  -0.793029             -1.166528             -0.043766   \n",
       "...                      ...                   ...                   ...   \n",
       "169995             -0.095021              0.175175              0.914245   \n",
       "169996             -0.733719             -0.285776             -0.323312   \n",
       "169997              0.744969             -0.965547              1.020598   \n",
       "169998              0.821976              0.351411              0.175762   \n",
       "169999             -1.017410              0.883248             -0.184107   \n",
       "\n",
       "        Personnality Trait 4  Concert Goer Age    Concert Goer ID  \\\n",
       "0                  -1.636806              29.0  concert_goer_1985   \n",
       "1                   0.610265              43.0  concert_goer_1874   \n",
       "2                  -1.260871              68.0   concert_goer_442   \n",
       "3                  -1.147251              17.0  concert_goer_1149   \n",
       "4                   0.969661              59.0   concert_goer_930   \n",
       "...                      ...               ...                ...   \n",
       "169995              0.357359              50.0   concert_goer_707   \n",
       "169996              0.641180              71.0  concert_goer_1373   \n",
       "169997              1.027389              27.0  concert_goer_1286   \n",
       "169998              1.455654              39.0  concert_goer_1845   \n",
       "169999              0.631731              46.0  concert_goer_1281   \n",
       "\n",
       "        Height (cm)  Concert Goer Country of Origin   Concert Enjoyment  \n",
       "0             140.0                        Paraguay       Did Not Enjoy  \n",
       "1             158.0             United Kingdom (UK)             Enjoyed  \n",
       "2             159.0  United States of America (USA)       Did Not Enjoy  \n",
       "3             150.0                          Canada  Worst Concert Ever  \n",
       "4             166.0             United Kingdom (UK)       Did Not Enjoy  \n",
       "...             ...                             ...                 ...  \n",
       "169995        180.0  United States of America (USA)       Did Not Enjoy  \n",
       "169996        143.0                        Bulgaria  Worst Concert Ever  \n",
       "169997        176.0                          Canada       Did Not Enjoy  \n",
       "169998        176.0                          Canada       Did Not Enjoy  \n",
       "169999        146.0                          Canada   Best Concert Ever  \n",
       "\n",
       "[170000 rows x 19 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "train_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data statistics\n",
    "value = train_data.mode().loc[0] # most frequent strategy\n",
    "\n",
    "# fill nan values\n",
    "train_data.fillna(value=value, inplace=True)\n",
    "test_data.fillna(value=value, inplace=True)\n",
    "\n",
    "# convert Concert ID to string\n",
    "train_data['Concert ID'] = train_data['Concert ID'].map(lambda x: str(x))\n",
    "test_data['Concert ID'] = test_data['Concert ID'].map(lambda x: str(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer, MinMaxScaler, StandardScaler, LabelEncoder\n",
    "\n",
    "\n",
    "# instantiate transformers\n",
    "mlb = MultiLabelBinarizer()\n",
    "mms = MinMaxScaler()\n",
    "ss = StandardScaler()\n",
    "le = LabelEncoder()\n",
    "\n",
    "\n",
    "# fit transformers on train data\n",
    "mlb.fit( train_data[['Band Name', 'Band Genre', 'Band Country of Origin', 'Concert ID', 'Concert Goer ID', 'Concert Goer Country of Origin']].to_numpy() )\n",
    "mms.fit( train_data[['Band Debut', 'Concert Attendance', 'Concert Goer Age', 'Height (cm)']].to_numpy() )\n",
    "ss.fit( train_data[['Personnality Trait 1', 'Personnality Trait 2', 'Personnality Trait 3', 'Personnality Trait 4']].to_numpy() )\n",
    "le.fit( train_data['Concert Enjoyment'].to_numpy() )\n",
    "\n",
    "\n",
    "# apply transformers on train data\n",
    "X_train = np.hstack((\n",
    "    mlb.transform( train_data[['Band Name', 'Band Genre', 'Band Country of Origin', 'Concert ID', 'Concert Goer ID', 'Concert Goer Country of Origin']].to_numpy() ),\n",
    "    mms.transform( train_data[['Band Debut', 'Concert Attendance', 'Concert Goer Age', 'Height (cm)']].to_numpy() ),\n",
    "    ss.transform( train_data[['Personnality Trait 1', 'Personnality Trait 2', 'Personnality Trait 3', 'Personnality Trait 4']].to_numpy() ),\n",
    "    train_data[['Inside Venue', 'Rain', 'Seated']].to_numpy(dtype=np.float32)\n",
    "))\n",
    "\n",
    "y_train = le.transform( train_data['Concert Enjoyment'].to_numpy() )\n",
    "\n",
    "\n",
    "# apply transformers on test data\n",
    "X_test = np.hstack((\n",
    "    mlb.transform( test_data[['Band Name', 'Band Genre', 'Band Country of Origin', 'Concert ID', 'Concert Goer ID', 'Concert Goer Country of Origin']].to_numpy() ),\n",
    "    mms.transform( test_data[['Band Debut', 'Concert Attendance', 'Concert Goer Age', 'Height (cm)']].to_numpy() ),\n",
    "    ss.transform( test_data[['Personnality Trait 1', 'Personnality Trait 2', 'Personnality Trait 3', 'Personnality Trait 4']].to_numpy() ),\n",
    "    test_data[['Inside Venue', 'Rain', 'Seated']].to_numpy(dtype=np.float32)\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import SparsePCA\n",
    "\n",
    "\n",
    "spca = SparsePCA(n_components=500, random_state=42)\n",
    "\n",
    "spca.fit( X_train )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# 0.9:0.1 (train:validation) split for model validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Torch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcertDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor]:\n",
    "        features = torch.tensor(self.X[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.y[idx], dtype=torch.long) # type long required for CrossEntropyLoss\n",
    "        return features, label\n",
    "\n",
    "\n",
    "train_dataset = ConcertDataset(X_train, y_train)\n",
    "val_dataset = ConcertDataset(X_val, y_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression # discriminative\n",
    "from sklearn.svm import LinearSVC # discriminant-based\n",
    "from sklearn.tree import DecisionTreeClassifier # decision tree\n",
    "\n",
    "\n",
    "class FNN3Classifier(nn.Module):\n",
    "    def __init__(self, in_features, num_nodes, out_features) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(in_features, num_nodes),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_nodes, num_nodes),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_nodes, out_features)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class FNN8Classifier(nn.Module):\n",
    "    def __init__(self, in_features, out_features) -> None:\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(in_features, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, out_features)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "logistic_classifier = LogisticRegression(penalty='l2', random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'max_iter': [1000, 10000, 100000]\n",
    "}\n",
    "\n",
    "# k-fold cross-validator\n",
    "k_fold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# hyper-parameter optimizer\n",
    "logistic_classifier_grid_search = GridSearchCV(\n",
    "    estimator=logistic_classifier,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=k_fold\n",
    ")\n",
    "\n",
    "# fit grid search\n",
    "logistic_classifier_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# prediction\n",
    "y_pred_logistic_classifier = logistic_classifier_grid_search.predict(X_val)\n",
    "\n",
    "# compute validation accuracy\n",
    "accuracy_logistic_classifier = accuracy_score(y_val, y_pred_logistic_classifier)\n",
    "\n",
    "# save grid search\n",
    "joblib.dump(logistic_classifier_grid_search, f'storage/logistic_classifier_acc{accuracy_logistic_classifier:.5f}.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Support Vector Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linear_svc = LinearSVC(penalty='l2', loss='squared_hinge', random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],\n",
    "    'max_iter': [1000, 10000, 100000]\n",
    "}\n",
    "\n",
    "# k-fold cross-validator\n",
    "k_fold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# hyper-parameter optimizer\n",
    "linear_svc_grid_search = GridSearchCV(\n",
    "    estimator=linear_svc,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=k_fold\n",
    ")\n",
    "\n",
    "# fit grid search\n",
    "linear_svc_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# prediction\n",
    "y_pred_linear_svc = linear_svc_grid_search.predict(X_val)\n",
    "\n",
    "# compute validation accuracy\n",
    "accuracy_linear_svc = accuracy_score(y_val, y_pred_linear_svc)\n",
    "\n",
    "# save grid search\n",
    "joblib.dump(linear_svc_grid_search, f'storage/linear_svc_acc{accuracy_linear_svc:.5f}.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "decision_tree_classifier = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [10, 20, 30, 40, 50]\n",
    "}\n",
    "\n",
    "# k-fold cross-validator\n",
    "k_fold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# hyper-parameter optimizer\n",
    "decision_tree_classifier_grid_search = GridSearchCV(\n",
    "    estimator=decision_tree_classifier,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=k_fold\n",
    ")\n",
    "\n",
    "# fit grid search\n",
    "decision_tree_classifier_grid_search.fit(X_train, y_train)\n",
    "\n",
    "# prediction\n",
    "y_pred_decision_tree_classifier = decision_tree_classifier_grid_search.predict(X_val)\n",
    "\n",
    "# compute validation accuracy\n",
    "accuracy_decision_tree_classifier = accuracy_score(y_val, y_pred_decision_tree_classifier)\n",
    "\n",
    "# save grid search\n",
    "joblib.dump(decision_tree_classifier_grid_search, f'storage/decision_tree_classifier_acc{accuracy_decision_tree_classifier:.5f}.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validation(val_dataloader: DataLoader, model: nn.Module, loss_fn: nn.CrossEntropyLoss) -> float:\n",
    "    \"\"\"Validation loop\n",
    "\n",
    "    Args:\n",
    "        val_dataloader (DataLoader)\n",
    "        model (nn.Module)\n",
    "        loss_fn (nn.CrossEntropyLoss)\n",
    "\n",
    "    Returns:\n",
    "        float: val loss\n",
    "    \"\"\"\n",
    "    size = len(val_dataloader.dataset)\n",
    "    num_batches = len(val_dataloader)\n",
    "    val_loss = 0.\n",
    "    correct = 0.\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in val_dataloader:\n",
    "            outputs = model(features)\n",
    "            val_loss += loss_fn(outputs, labels).item()\n",
    "            correct += (outputs.argmax(1) == labels).type(torch.float).sum().item()\n",
    "    \n",
    "    val_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Validation Error:\\n    Accuracy: {(100*correct):>.2f}%\\n    Loss: {val_loss:>.8f}\\n\")\n",
    "    return val_loss, correct\n",
    "\n",
    "\n",
    "def epoch(train_dataloader: DataLoader, val_dataloader: DataLoader, \n",
    "            model: nn.Module, loss_fn: nn.CrossEntropyLoss, optimizer: torch.optim.Adam) -> tuple:\n",
    "    \"\"\"Train one epoch\n",
    "\n",
    "    Args:\n",
    "        train_dataloader (DataLoader)\n",
    "        val_dataloader (DataLoader)\n",
    "        model (nn.Module)\n",
    "        loss_fn (nn.CrossEntropyLoss)\n",
    "        optimizer (torch.optim.Adam)\n",
    "\n",
    "    Returns:\n",
    "        tuple\n",
    "    \"\"\"\n",
    "    epoch_train_losses = []\n",
    "    epoch_train_steps = []\n",
    "\n",
    "    size = len(train_dataloader.dataset)\n",
    "\n",
    "    for step, (features, labels) in enumerate(train_dataloader):\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(features)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss = loss.item()\n",
    "\n",
    "        epoch_train_losses.append(loss)\n",
    "        epoch_train_steps.append(step)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            current = step * len(features)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        \n",
    "        if step == 0:\n",
    "\n",
    "            # forward pass\n",
    "            val_loss, correct = validation(val_dataloader, model, loss_fn)\n",
    "        \n",
    "            epoch_val_loss = val_loss\n",
    "            epoch_val_accuracy = correct\n",
    "            epoch_val_step= step\n",
    "    \n",
    "    return epoch_train_losses, epoch_train_steps, epoch_val_loss, epoch_val_accuracy, epoch_val_step\n",
    "\n",
    "\n",
    "def train(num_epochs: int, train_dataloader: DataLoader, val_dataloader: DataLoader, \n",
    "            model: nn.Module, loss_fn: nn.MSELoss, optimizer: torch.optim.Adam) -> tuple:\n",
    "    \"\"\"Train loop\n",
    "\n",
    "    Args:\n",
    "        num_epochs (int)\n",
    "        train_dataloader (DataLoader)\n",
    "        val_dataloader (DataLoader)\n",
    "        model (nn.Module)\n",
    "        loss_fn (nn.MSELoss)\n",
    "        optimizer (torch.optim.Adam)\n",
    "\n",
    "    Returns:\n",
    "        tuple\n",
    "    \"\"\"\n",
    "    train_losses = []\n",
    "    train_steps = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    val_steps = []\n",
    "    epoch_last_step = 0\n",
    "\n",
    "    best_val_accuracy = 0\n",
    "\n",
    "    try:\n",
    "        for t in range(num_epochs):\n",
    "            print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "\n",
    "            epoch_train_losses, epoch_train_steps, \\\n",
    "                epoch_val_loss, epoch_val_accuracy, epoch_val_step = \\\n",
    "                    epoch(train_dataloader, val_dataloader, model, loss_fn, optimizer)\n",
    "            \n",
    "            if epoch_val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = epoch_val_accuracy\n",
    "                best_state = model.state_dict()\n",
    "\n",
    "            train_losses += epoch_train_losses\n",
    "            train_steps += [step + epoch_last_step for step in epoch_train_steps]\n",
    "\n",
    "            val_losses.append( epoch_val_loss )\n",
    "            val_accuracies.append( epoch_val_accuracy )\n",
    "            val_steps.append( epoch_val_step + epoch_last_step )\n",
    "\n",
    "            epoch_last_step = train_steps[-1]\n",
    "\n",
    "    # early stopping\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    return train_losses, train_steps, val_losses, val_accuracies, val_steps, best_val_accuracy, best_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.392023  [    0/153000]\n",
      "Validation Error:\n",
      "    Accuracy: 40.27%\n",
      "    Loss: 1.38555557\n",
      "\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'best_state' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [37], line 23\u001b[0m\n\u001b[1;32m     18\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mLEARNING_RATE)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# train model\u001b[39;00m\n\u001b[1;32m     22\u001b[0m train_losses, train_steps, val_losses, val_accuracies, val_steps, best_val_accuracy, best_state \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m---> 23\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# save model weights\u001b[39;00m\n\u001b[1;32m     27\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(best_state, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstorage/FNN3_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mN_NODES\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_bs\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBATCH_SIZE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_lr\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLEARNING_RATE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_ep\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mN_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_acc\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_val_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.5f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn [35], line 129\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(num_epochs, train_dataloader, val_dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m--> 129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_losses, train_steps, val_losses, val_accuracies, val_steps, best_val_accuracy, \u001b[43mbest_state\u001b[49m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'best_state' referenced before assignment"
     ]
    }
   ],
   "source": [
    "\n",
    "# FNN hyper-parameters\n",
    "N_NODES = 1024\n",
    "\n",
    "# training hyper-parameters\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 1e-4\n",
    "N_EPOCHS = 10\n",
    "\n",
    "# DataLoader wraps an iterable around the Dataset\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "model = FNN3Classifier(in_features=X_train.shape[1], num_nodes=N_NODES, out_features=4).to('cpu')\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "# train model\n",
    "train_losses, train_steps, val_losses, val_accuracies, val_steps, best_val_accuracy, best_state = \\\n",
    "    train(N_EPOCHS, train_dataloader, val_dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "\n",
    "# save model weights\n",
    "torch.save(best_state, f'storage/FNN3_{N_NODES}_bs{BATCH_SIZE}_lr{LEARNING_RATE}_ep{N_EPOCHS}_acc{best_val_accuracy:.5f}.pth')\n",
    "\n",
    "\n",
    "# save stats\n",
    "stats = {   \n",
    "    'train_steps': train_steps,\n",
    "    'train_losses': train_losses,\n",
    "    'val_steps': val_steps,\n",
    "    'val_accuracies': val_accuracies,\n",
    "    'val_losses': val_losses\n",
    "}\n",
    "\n",
    "with open(f'storage/FNN3_{N_NODES}_bs{BATCH_SIZE}_lr{LEARNING_RATE}_ep{N_EPOCHS}_acc{best_val_accuracy:.5f}.pkl', 'wb') as handle:\n",
    "    pickle.dump(stats, handle)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_NODES = 1024\n",
    "\n",
    "model = FNN3Classifier(in_features=X_train.shape[1], num_nodes=N_NODES, out_features=4)\n",
    "model.load_state_dict( torch.load('storage/FNN3_1024_bs256_lr0.0001_ep10_acc0.68035.pth') )\n",
    "model.eval()\n",
    "\n",
    "outputs = model( torch.tensor(X_test, dtype=torch.float32) )\n",
    "predictions = le.inverse_transform( outputs.argmax(axis=1) )\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'Id': test_data['Id'],\n",
    "    'Predicted': predictions\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submissions/FNN3_1024_bs256_lr0.0001_ep10_acc0.68035.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(test_dataloader: DataLoader, model: nn.Module) -> float:\n",
    "    \"\"\"Validation loop\n",
    "\n",
    "    Args:\n",
    "        test_dataloader (DataLoader)\n",
    "        model (nn.Module)\n",
    "\n",
    "    Returns:\n",
    "        float: val loss\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for feature_tensors, _ in test_dataloader:\n",
    "            outputs = model(feature_tensors)\n",
    "            predictions.append(outputs.argmax(1))\n",
    "    \n",
    "    predictions = torch.cat(predictions, dim=0)\n",
    "\n",
    "    return predictions.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ConcertExperience_70055</td>\n",
       "      <td>Best Concert Ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ConcertExperience_34799</td>\n",
       "      <td>Enjoyed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ConcertExperience_100410</td>\n",
       "      <td>Enjoyed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ConcertExperience_106446</td>\n",
       "      <td>Did Not Enjoy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ConcertExperience_127249</td>\n",
       "      <td>Did Not Enjoy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29995</th>\n",
       "      <td>ConcertExperience_82288</td>\n",
       "      <td>Did Not Enjoy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29996</th>\n",
       "      <td>ConcertExperience_27139</td>\n",
       "      <td>Enjoyed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29997</th>\n",
       "      <td>ConcertExperience_197434</td>\n",
       "      <td>Enjoyed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29998</th>\n",
       "      <td>ConcertExperience_166029</td>\n",
       "      <td>Worst Concert Ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29999</th>\n",
       "      <td>ConcertExperience_24025</td>\n",
       "      <td>Worst Concert Ever</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Id           Predicted\n",
       "0       ConcertExperience_70055   Best Concert Ever\n",
       "1       ConcertExperience_34799             Enjoyed\n",
       "2      ConcertExperience_100410             Enjoyed\n",
       "3      ConcertExperience_106446       Did Not Enjoy\n",
       "4      ConcertExperience_127249       Did Not Enjoy\n",
       "...                         ...                 ...\n",
       "29995   ConcertExperience_82288       Did Not Enjoy\n",
       "29996   ConcertExperience_27139             Enjoyed\n",
       "29997  ConcertExperience_197434             Enjoyed\n",
       "29998  ConcertExperience_166029  Worst Concert Ever\n",
       "29999   ConcertExperience_24025  Worst Concert Ever\n",
       "\n",
       "[30000 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_inverse_map = {\n",
    "    0: 'Worst Concert Ever',\n",
    "    1: 'Did Not Enjoy',\n",
    "    2: 'Enjoyed',\n",
    "    3: 'Best Concert Ever'\n",
    "}\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "predictions = test(test_dataloader, model)\n",
    "predictions = pd.Series(predictions).apply( lambda x: label_inverse_map[x] )\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'Id': test_data['Id'],\n",
    "    'Predicted': predictions\n",
    "})\n",
    "\n",
    "submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submissions/FNN_100_bs32_lr0.001_ep12.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0e3b4952062f618dbf0490f0178ad443c32c089b4a05be38315e03675ee3253a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
